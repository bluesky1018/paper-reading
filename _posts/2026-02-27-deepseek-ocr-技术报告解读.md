---
title: "DeepSeek-OCR: Contexts Optical Compression"
date: 2026-02-27 11:30:00 +0800
categories: [OCR, VLM, Compression]
tags: [deepseek, ocr, vision-language-model, optical-compression, token-compression, MoE]
math: true
---

## 基本信息

- **作者**: Haoran Wei, Yaofeng Sun, Yukun Li
- **机构**: DeepSeek-AI
- **发表**: arXiv 2025
- **arXiv**: [2510.18234](https://arxiv.org/abs/2510.18234)
- **代码**: [github.com/deepseek-ai/DeepSeek-OCR](http://github.com/deepseek-ai/DeepSeek-OCR)

## 一句话总结

提出了**上下文光学压缩**范式，通过视觉编码器将文本信息压缩为少量视觉Token，实现了**10倍无损压缩**（97%准确率）和**20倍可接受压缩**（60%准确率），为长上下文LLM的存储与处理开辟了全新方向。

## 背景与动机

### 长上下文LLM的算力困境

当前大语言模型面临严峻的计算挑战：

1. **二次方复杂度**: 注意力机制的计算量与序列长度呈 $O(n^2)$ 关系
2. **显存瓶颈**: 长序列导致KV Cache占用巨大，成为主要瓶颈
3. **推理延迟**: 预填充和生成阶段都随序列长度增加而显著变慢

### 核心洞察：一张图胜过千言万语

承载相同信息的图像可以用**远少于**等效数字文本的Token表示：

| 表示方式 | Token数量 | 说明 |
|----------|-----------|------|
| 1000个文本字符 | ~1000个文本Token | 标准文本编码 |
| 承载同样信息的图像 | 64-256个视觉Token | 光学压缩 |
| **压缩比** | **4-16倍** | 潜力巨大 |

这一洞察促使作者重新审视VLM：从LLM-centric的视角，探索如何利用视觉模态提升LLM处理文本信息的效率。

### 为什么选择OCR作为突破口

OCR任务作为视觉与语言之间的中间模态，为验证**压缩-解压**范式提供了理想试验场：
- 建立视觉与文本表示之间的自然映射关系
- 提供定量评估指标（OCR解码准确率）
- 连接历史长上下文压缩与LLM记忆遗忘机制的研究

## 核心贡献

### 1. DeepEncoder：低激活内存的高效视觉编码器

DeepEncoder是DeepSeek-OCR的核心，由三个串联组件构成：

**架构设计**:
```
输入图像 (1024×1024)
    ↓ [Patch Embed]
4096个Patch Tokens
    ↓ [SAM-base, 窗口注意力, 80M参数]
4096个Tokens (低激活内存)
    ↓ [16×卷积压缩器]
256个压缩Tokens
    ↓ [CLIP-large, 全局注意力, 300M参数]
256个视觉Token输出
```

**关键创新点**:
- **窗口注意力主导**: SAM组件处理大量Token但激活内存低
- **Token压缩器**: 16倍下采样，在进入全局注意力前减少Token数
- **全局注意力精炼**: CLIP组件在可控激活内存下注入高级语义

### 2. 多分辨率支持体系

为满足不同压缩比需求，设计了完整的分辨率模式：

| 模式 | 分辨率 | Token数 | 处理方式 | 适用场景 |
|------|--------|---------|----------|----------|
| Tiny | 512×512 | 64 | Resize | 简单文档 |
| Small | 640×640 | 100 | Resize | 标准页面 |
| Base | 1024×1024 | 256 | Padding | 复杂版面 |
| Large | 1280×1280 | 400 | Padding | 高精度需求 |
| Gundam | 动态 | n×100+256 | Tiling | 超高分辨率 |

**动态分辨率 (Gundam模式)**:
- 局部视图：n × 640×640 tiles（类似InternVL2.0的切片方法）
- 全局视图：1024×1024 overview
- 适合报纸等超高分辨率输入（Token数可控在2-9个切片）

### 3. DeepSeek-3B-MoE解码器

- **总参数**: 3B
- **激活参数**: 570M（6/64路由专家 + 2共享专家）
- **核心优势**: 3B模型的表达能力，500M模型的推理效率

解码过程形式化为：

$$f_{\text{dec}}: \mathbb{R}^{n \times d_{\text{latent}}} \rightarrow \mathbb{R}^{N \times d_{\text{text}}}; \quad \hat{\mathbf{X}} = f_{\text{dec}}(\mathbf{Z}) \quad \text{where } n \leq N$$

即从少量视觉Token解码出更多文本Token。

### 4. 全面的数据工程

**OCR 1.0数据（文档级）**:
- 3000万页PDF，覆盖约100种语言
- 中文2500万页，其他语言500万页
- 粗标注：fitz直接提取（全文）
- 细标注：200万页中英高质量版面标注

**OCR 2.0数据（结构化解析）**:
- 图表：1000万张（pyecharts + matplotlib渲染）
- 化学公式：500万张（SMILES + RDKit）
- 平面几何：100万张（Slow Perception方法）

**数据配比**:
- OCR数据：70%
- 通用视觉数据：20%（保留视觉接口）
- 纯文本数据：10%（维持语言能力）

## 实验结果

### 1. 视觉-文本压缩比研究（Fox基准）

在600-1300 Token的英文文档上测试：

| 文本Token数 | 视觉Token=64 | 压缩比 | 准确率 | 视觉Token=100 | 压缩比 | 准确率 |
|------------|--------------|--------|--------|---------------|--------|--------|
| 600-700 | 64 | 10.5× | **96.5%** | 100 | 6.7× | **98.5%** |
| 900-1000 | 64 | 15.1× | 85.9% | 100 | 9.7× | **96.8%** |
| 1200-1300 | 64 | 19.7× | 59.1% | 100 | 12.6× | **87.1%** |

**关键发现**:
- **10倍压缩内**：准确率~97%，接近无损
- **20倍压缩**：准确率仍保持~60%
- 超过10倍后性能下降，原因包括版面复杂度和分辨率限制

### 2. 实际OCR性能（OmniDocBench）

| 模型 | 视觉Tokens | 英文Overall | 中文Overall |
|------|-----------|-------------|-------------|
| GOT-OCR2.0 | 256 | 0.287 | 0.411 |
| MinerU2.0 | ~6790 | 0.133 | 0.238 |
| Gemini2.5-Pro | - | 0.148 | 0.212 |
| **DeepSeek-OCR (Small)** | **100** | **0.221** | **0.284** |
| **DeepSeek-OCR (Base)** | **256(182有效)** | **0.137** | **0.240** |
| **DeepSeek-OCR (Gundam)** | **795** | **0.127** | **0.181** |
| **DeepSeek-OCR (Gundam-M)** | **1853** | **0.123** | **0.157** |

*注：数值为Edit Distance，越小越好*

**核心成就**:
- 仅用**100 Token**超越GOT-OCR2.0（256 Token）
- 少于**800 Token**超越MinerU2.0（~6790 Token）
- 在端到端模型中达到SOTA，同时使用最少视觉Token

### 3. 不同文档类型分析

| 模式 | Book | Slides | Report | Paper | Newspaper |
|------|------|--------|--------|-------|-----------|
| Tiny (64) | 0.147 | **0.116** | 0.207 | 0.201 | 0.940 |
| Small (100) | 0.085 | 0.111 | **0.079** | 0.107 | 0.744 |
| Gundam (795) | 0.035 | 0.085 | 0.289 | 0.059 | **0.122** |

**洞察**:
- **幻灯片**：64 Token即可达到很好效果
- **书籍/报告**：100 Token即可胜任
- **报纸**：需要Gundam模式（Token数达4-5K）

### 4. 与现有视觉编码器的对比

| 类型 | 代表模型 | 高分辨率 | 低激活内存 | 少Token | 缺点 |
|------|----------|----------|------------|---------|------|
| 双塔架构 | Vary | ✓ | ✓ | ✗ | 双图预处理复杂 |
| 切片方法 | InternVL2.0 | ✓ | ✓ | ✗ | Token过度碎片化 |
| 自适应分辨率 | Qwen2-VL | ✓ | ✗ | ✓ | 大图像显存溢出 |
| **DeepEncoder** | **DeepSeek-OCR** | **✓** | **✓** | **✓** | **-** |

## 个人思考

### 为什么光学压缩能work

1. **信息统计冗余**：自然语言存在大量可压缩的统计规律
2. **视觉并行处理**：图像CNN/Transformer擅长并行处理空间信息
3. **端到端学习**：神经网络可以学习隐式的字典压缩编码

### 潜在应用方向

#### 方向1：多轮对话历史压缩

将历史对话渲染为图像进行10倍压缩：
```
[当前轮次对话] --文本Token--> LLM
    ↓
[第N-1轮历史] --光学压缩--> 100视觉Tokens（10×压缩）
    ↓
[第N-2轮历史] --更高压缩--> 50视觉Tokens（20×压缩）
```

#### 方向2：模拟人类记忆遗忘机制

光学压缩可以模拟人类记忆的自然衰减：
- **近期信息**：高分辨率，高保真（如100 Token）
- **中期信息**：中等压缩（如64 Token）
- **远期信息**：高度压缩，逐渐模糊（如32 Token）

这与人类记忆的遗忘曲线高度相似——近期记忆清晰，远期记忆模糊。

#### 方向3：理论上无限上下文架构

分层压缩架构：
| 层级 | 表示形式 | 压缩比 | 信息保真度 |
|------|----------|--------|------------|
| 当前上下文 | 文本Token | 1× | 100% |
| 近期历史 | 光学压缩（Base） | 10× | ~97% |
| 中期历史 | 光学压缩（Small） | 12× | ~87% |
| 远期历史 | 光学压缩（Tiny） | 20× | ~60% |

这种分层架构理论上可以实现**无限上下文**，同时保持可控的计算成本。

### 局限与未来工作

| 当前局限 | 可能的改进方向 |
|----------|----------------|
| 仅验证OCR场景 | 扩展至数字-光学文本交错预训练 |
| 缺乏长程检索测试 | needle-in-haystack测试验证信息检索 |
| 格式差异影响评估 | 标准化输出格式评估 |
| 解码准确率上限 | 更大的LLM解码器，专用预训练优化 |

## 相关阅读

### 基础工作
- **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** - Transformer基础架构
- **[CLIP](https://arxiv.org/abs/2103.00020)** - 对比语言-图像预训练
- **[SAM](https://arxiv.org/abs/2304.02643)** - 分割一切模型

### OCR相关工作
- **[GOT-OCR2.0](https://arxiv.org/abs/2410.04756)** - OCR 2.0先驱工作
- **[Vary](https://arxiv.org/abs/2312.06109)** - 扩展视觉词表
- **[Nougat](https://arxiv.org/abs/2308.13418)** - 学术文档OCR

### 多模态大模型
- **[InternVL2](https://arxiv.org/abs/2404.16821)** - 切片式高分辨率处理
- **[Qwen2-VL](https://arxiv.org/abs/2409.12191)** - 自适应分辨率
- **[NaViT](https://arxiv.org/abs/2307.06304)** - 任意分辨率ViT
- **[DeepSeek-VL2](https://arxiv.org/abs/2412.14342)** - DeepSeek多模态系列

### 压缩与效率
- **[DeepSeekMoE](https://arxiv.org/abs/2401.06066)** - MoE架构设计
- **[Mixture of Experts](https://arxiv.org/abs/2412.19437)** - DeepSeek-V3技术报告

## 引用

```bibtex
@article{wei2025deepseekocr,
  title={DeepSeek-OCR: Contexts Optical Compression},
  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},
  journal={arXiv preprint arXiv:2510.18234},
  year={2025}
}
```

---

*本文解读基于DeepSeek-OCR论文（arXiv:2510.18234），如有侵权请联系删除。*
