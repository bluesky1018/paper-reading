---
title: "Segment Anything (SAM): 图像分割的Foundation Model"
date: 2026-02-27 16:20:00 +0800
categories: [Computer Vision, Segmentation, Foundation Model]
tags: [sam, segment-anything, foundation-model, zero-shot, image-segmentation, meta-ai]
math: true
---

## 基本信息

- **作者**: Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick
- **机构**: Meta AI (FAIR)
- **发表**: arXiv 2023
- **arXiv**: [2304.02643](https://arxiv.org/abs/2304.02643)
- **项目主页**: [segment-anything.com](https://segment-anything.com)
- **代码/模型**: [github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)

## 一句话总结

提出了**可提示的图像分割任务**和**Segment Anything Model (SAM)**，通过在迄今最大的分割数据集（SA-1B，1100万张图像、10亿+掩码）上训练，实现了强大的**零样本迁移能力**，开启了计算机视觉的Foundation Model时代。

## 背景与动机

### 传统分割模型的局限

计算机视觉中的分割任务长期面临以下困境：

1. **任务碎片化**: 语义分割、实例分割、全景分割各自为政，模型无法通用
2. **数据依赖严重**: 需要大量人工标注数据，标注成本高企
3. **泛化能力弱**: 训练好的模型难以迁移到新领域或新任务

### NLP Foundation Model的启示

NLP领域通过GPT、BERT等模型展示了Foundation Model的强大潜力：

| 特性 | NLP Foundation Model | 传统CV分割模型 |
|------|---------------------|---------------|
| 预训练数据规模 | 万亿级Token | 百万级标注 |
| 零样本能力 | 强（提示工程） | 几乎无 |
| 任务通用性 | 一个模型处理多种任务 | 每个任务单独训练 |
| 迁移学习 | 提示即可适配新任务 | 需要微调甚至重训练 |

**核心问题**: 计算机视觉能否拥有类似NLP的Foundation Model？

### SAM的核心洞察

要实现CV的Foundation Model，需要解决三个关键问题：

1. **任务定义**: 什么样的任务能够覆盖广泛的分割需求？
2. **模型架构**: 如何设计支持灵活提示的模型？
3. **数据规模**: 如何构建足够大的训练数据集？

SAM的回答是：**可提示的分割任务 + 提示驱动架构 + 数据引擎循环**。

## 核心贡献

### 1. 可提示的分割任务（Promptable Segmentation Task）

**问题定义**:
给定图像和任意**提示（Prompt）**，模型返回有效的分割掩码。

**提示形式多样化**:

| 提示类型 | 描述 | 示例 |
|---------|------|------|
| **稀疏提示** | 点、框 | 前景/背景点、边界框 |
| **粗粒度提示** | 掩码 | 粗略的掩码区域 |
| **文本提示** | 自然语言 | "猫"、"天空" |

**关键设计**:
- 提示是**模糊的**: 同一个提示可能对应多个有效掩码（如一个点可能在多个物体上）
- 模型需要输出**合理的**分割结果，即使提示不明确

### 2. Segment Anything Model (SAM) 架构

SAM由三个核心组件构成，形成一个灵活的提示驱动系统：

```
输入图像
    ↓
┌─────────────────┐
│  Image Encoder  │  ← 处理整张图像一次，输出图像嵌入
│  (ViT-based)    │     基于MAE预训练的Vision Transformer
└────────┬────────┘
         ↓ 图像嵌入 (Image Embedding)
┌─────────────────┐
│  Prompt Encoder │  ← 处理各种提示
│                 │     • 点/框: 位置编码
│                 │     • 掩码: 卷积下采样
│                 │     • 文本: CLIP编码器
└────────┬────────┘
         ↓ 提示嵌入 (Prompt Embedding)
┌─────────────────┐
│   Mask Decoder  │  ← 轻量级Transformer解码器
│                 │     输出掩码和置信度分数
└─────────────────┘
         ↓
    分割掩码 + 置信度
```

**架构亮点**:

| 组件 | 设计选择 | 优势 |
|------|---------|------|
| Image Encoder | ViT-H/16 (MAE预训练) | 强大的视觉表征 |
| Prompt Encoder | 位置编码 + 卷积 | 支持多种提示形式 |
| Mask Decoder | 两层Transformer | 轻量、快速推理 |
| 歧义处理 | 输出3个掩码 + 置信度 | 应对模糊提示 |

**解决提示歧义性**:
当提示不明确时（如一个点在多个物体边界），SAM输出**3个掩码**及其置信度：
- 整体掩码（如一群物体）
- 部分掩码（如单个物体）
- 子部分掩码（如物体的某个部件）

### 3. SA-1B: 迄今为止最大的分割数据集

**数据规模对比**:

| 数据集 | 图像数量 | 掩码数量 | 平均每图掩码 |
|--------|---------|---------|-------------|
| COCO | 12万张 | 88万个 | ~7 |
| Open Images | 900万张 | 860万个 | ~1 |
| LVIS | 10万张 | 120万个 | ~12 |
| **SA-1B** | **1100万张** | **11亿个** | **~100** |

**数据引擎（Data Engine）**: 三步迭代策略

```
第一阶段: 辅助人工标注
    专业标注员使用SAM交互式标注
    ↓ 训练初始模型
第二阶段: 半自动标注  
    模型自动生成部分掩码，人工补充困难部分
    ↓ 提升模型能力
第三阶段: 全自动标注
    模型全自动生成高质量掩码，无需人工干预
    ↓ 最终数据集
    SA-1B (11M图像, 1.1B掩码)
```

**数据质量保障**:
- 图像来自11M张授权且隐私保护的图片
- 平均每图约100个掩码，覆盖完整
- 经过专业验证，掩码质量高

## 技术架构图解

### 图1: SAM 三支柱设计总览

论文图1清晰地展示了SAM项目的三大核心组件及其相互关系：

**图1(a) Task - 可提示分割任务**
- 输入：图像 + 分割提示
- 提示形式多样化：点（前景/背景）、框、掩码、文本描述
- 输出：valid mask（有效分割掩码）
- 关键特性：一个提示可能对应多个有效掩码（歧义性）

**图1(b) Model - Segment Anything Model 架构**
- **Image Encoder**:  heavyweight 图像编码器（ViT-based）
- **Prompt Encoder**: 轻量级提示编码器
- **Lightweight Mask Decoder**: 轻量级掩码解码器
- 设计理念：图像编码器重量级（处理一次），提示相关组件轻量级（支持实时交互）

**图1(c) Data - 数据引擎与SA-1B数据集**
- 上半部分：数据引擎循环（model → annotate → data → train → model）
- 下半部分：SA-1B数据集统计
  - 1+ billion masks（超过10亿掩码）
  - 11 million images（1100万张图像）
  - Privacy respecting（隐私保护）
  - Licensed images（授权图像）

**整体设计理念**：Task、Model、Data三者相互支撑，形成闭环——可提示任务定义指导模型设计，模型能力驱动数据引擎收集更多数据，大数据集反过来训练出更强的模型。

### 图4: SAM 详细数据流

论文图4深入展示了SAM内部的完整数据流动过程：

**数据流分解**：

1. **Image → Image Encoder**
   - 输入：原始图像（如示例中的剪刀图像）
   - 处理：通过 heavyweight Image Encoder（ViT）
   - 输出：Image Embedding（图像嵌入，图中显示为多个白色方块表示的feature map）

2. **Prompt → Prompt Encoder**
   - 支持多种提示输入：
     - **Mask**（掩码）：通过卷积层（图中标注"conv"）处理
     - **Points**（点）：前景/背景点击
     - **Box**（框）：边界框
     - **Text**（文本）：自然语言描述

3. **特征融合与解码**
   - Image Embedding 与 Prompt Embedding 在 Mask Decoder 中融合
   - 图中显示圆形符号（⊕）表示特征融合/交互
   - 通过轻量级Transformer解码器处理

4. **多掩码输出机制**（歧义性处理）
   - 输出3个不同的 valid masks（对应图中右侧3个剪刀分割结果）
   - 每个掩码配有 score（置信度分数）
   - 这解决了提示歧义问题：当提示不明确时，输出多个合理候选供选择

**关键设计洞察**：
- **计算效率**：Image Encoder只运行一次，后续提示交互只需重新运行轻量级的Prompt Encoder和Mask Decoder
- **实时性**： amortized real-time speed（摊销实时速度）
- **灵活性**：多种提示形式可以组合使用

### 架构图的设计哲学

从这两张架构图可以读出SAM的核心设计哲学：

| 设计原则 | 图1体现 | 图4体现 |
|---------|--------|--------|
| **解耦** | Task/Model/Data三模块独立 | Image/Prompt/Decoder分离 |
| **效率** | Data Engine自动化标注 | Image Encoder只算一次 |
| **灵活** | 多种提示形式 | 多模态提示输入 |
| **鲁棒** | 零样本迁移 | 多掩码应对歧义 |

### 4. 零样本迁移能力

SAM的核心价值在于其强大的零样本（Zero-Shot）能力：

**测试任务与结果**:

| 任务 | 数据集 | SAM表现 | 对比方法 |
|------|--------|---------|---------|
| 边缘检测 | BSDS500 | 0.768 (ODS) | 接近HED监督方法 |
| 实例分割 | COCO | 单点提示可达监督方法水平 | ViTDet |
| 交互式分割 | DAVIS | 超越RITM等专用方法 | SOTA |
| 立体分割 | KITTI | 零样本即可工作 | - |
| 伪装物体 | COD10K | 显著优于专用方法 | - |

**关键发现**:
- **提示越多，性能越好**: 增加点/框提示可显著提升分割质量
- **单点提示已具竞争力**: 仅凭一个点击即可达到监督方法水平
- **跨域泛化强**: 在训练时未见过的领域（如显微镜图像）也能工作

## 技术细节

### 图像编码器 (Image Encoder)

- **架构**: Vision Transformer (ViT-H/16)
- **预训练**: Masked Autoencoder (MAE)
- **输入**: 1024×1024 图像
- **输出**: 64×64×256 图像嵌入
- **特点**: 处理整张图像一次，后续提示交互无需重新计算

### 提示编码器 (Prompt Encoder)

**稀疏提示（点/框）**:
- 使用正弦位置编码
- 前景/背景点用不同可学习嵌入区分
- 框用左上角和右下角点编码对表示

**密集提示（掩码）**:
- 输入掩码与图像下采样4倍对齐
- 通过两个2×2卷积层（输出通道4和16）
- 与图像嵌入逐元素相加

### 掩码解码器 (Mask Decoder)

- **架构**: 两层标准Transformer解码器 + 动态掩码预测头
- **创新**: 使用**双向交叉注意力**
  - 提示Token ←→ 图像嵌入
  - 提示Token ←→ 输出Token
- **输出**: 3个掩码（应对歧义）+ IoU置信度分数

**上采样策略**:
- 输出Token与图像嵌入进行点积，得到低分辨率掩码
- 通过两个转置卷积上采样到4×原图分辨率
- 与原始图像尺寸对齐输出

## 实验结果与分析

### 1. 模型变体对比

SAM提供三种模型规模，满足不同需求：

| 模型 | 图像编码器 | 参数量 | 运行时间(CPU) |
|------|-----------|--------|--------------|
| SAM-ViT-B | ViT-B | 91M | ~2s |
| SAM-ViT-L | ViT-L | 308M | ~3s |
| SAM-ViT-H | ViT-H (默认) | 636M | ~5s |

### 2. 提示数量与性能

在COCO实例分割任务上：

| 提示方式 | AP (Average Precision) |
|---------|----------------------|
| 单点（中心点） | 46.3 |
| 1个随机点 | 47.1 |
| 3个随机点 | 65.3 |
| 5个随机点 | 69.1 |
| 边界框 | 71.1 |

**结论**: 简单的点/框提示即可达到接近全监督方法的性能。

### 3. 与其他方法的比较

**交互式分割**:
SAM在交互式分割任务上超越了RITM等专用方法，成为新的SOTA。

**边缘检测**:
SAM在BSDS500数据集上零样本边缘检测性能接近专门训练的HED方法。

**视频分割**:
在DAVIS视频分割基准上，SAM展现出强大的时序一致性。

## 个人思考

### 为什么SAM能work

1. **任务定义的普适性**: "可提示的分割"覆盖了几乎所有分割需求
2. **数据飞轮效应**: 模型辅助标注 → 更多数据 → 更好模型 → 更好标注工具
3. **架构的灵活性**: 图像编码器与提示编码器解耦，支持任意组合
4. **歧义处理的实用性**: 输出多个候选，让用户/下游任务选择

### SAM的局限与未来方向

| 当前局限 | 可能的改进方向 |
|---------|---------------|
| 文本提示相对较弱 | 更强的视觉-语言对齐 |
| 细粒度边界可能不够精确 | 引入边缘 refinement 模块 |
| 实时性有待提升 | 知识蒸馏、模型压缩 |
| 3D/视频支持有限 | 扩展到时空维度 |

### 对CV领域的影响

SAM标志着计算机视觉进入Foundation Model时代：

- **数据标注范式变革**: 从人工标注转向模型辅助标注
- **下游任务开发加速**: 零样本能力大幅降低新任务开发成本
- **多模态融合基础**: SAM可作为视觉 backbone 与LLM结合
- **产业化落地加速**: 分割任务的门槛大幅降低

### 与DeepSeek-OCR的关联

有趣的是，SAM的架构思想与DeepSeek-OCR有相似之处：

| SAM | DeepSeek-OCR |
|-----|-------------|
| Image Encoder + Prompt Encoder + Decoder | Image Encoder (DeepEncoder) + Decoder |
| 提示驱动分割 | 光学压缩重建 |
| 支持多种提示形式 | 支持多种分辨率模式 |
| 歧义性输出多个掩码 | 压缩比与质量权衡 |

两者都体现了**Foundation Model**的设计理念：
- 统一的编码器处理输入
- 灵活的机制支持多种任务
- 大规模预训练 + 零样本迁移

## 相关阅读

### SAM系列工作
- **[SAM 2](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)**: 扩展到视频分割
- **[SAM-Med2D](https://github.com/OpenGVLab/SAM-Med2D)**: 医学图像分割适配
- **[MobileSAM](https://github.com/ChaoningZhang/MobileSAM)**: 轻量化版本

### Foundation Model相关
- **[CLIP](https://arxiv.org/abs/2103.00020)** - 视觉-语言预训练
- **[DINOv2](https://arxiv.org/abs/2304.07193)** - 自监督视觉表征
- **[GPT-4V](https://openai.com/research/gpt-4v-system-card)** - 多模态大模型

### 分割基础工作
- **[Mask R-CNN](https://arxiv.org/abs/1703.06870)** - 实例分割经典方法
- **[Mask2Former](https://arxiv.org/abs/2112.01527)** - 通用分割架构
- **[DETR](https://arxiv.org/abs/2005.12872)** - 端到端目标检测与分割

### 数据引擎相关
- **[DALL-E](https://openai.com/research/dall-e)** - 生成式数据增强
- **[LAION-5B](https://laion.ai/blog/laion-5b/)** - 大规模图像-文本数据集

## 引用

```bibtex
@article{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}
```

---

*本文解读基于SAM论文（arXiv:2304.02643），如有侵权请联系删除。*
